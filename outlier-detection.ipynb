{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b624b2",
   "metadata": {},
   "source": [
    "## Data Ingestion and Setup\n",
    "\n",
    "This block of code sets up the Spark environment and reads in the initial purchase order dataset for anomaly detection.\n",
    "\n",
    "- **Spark Session**: Starts a Spark session named \"AnomalyDetection\".\n",
    "- **Imports**: Brings in essential PySpark functions (e.g. aggregation, transformation) and external libraries (Pandas, NumPy, Scikit-learn) for machine learning.\n",
    "- **File Loading**: Reads a CSV file from a local data directory as the primary dataset (`main_df`). This setup is adaptable—data sources can be easily swapped for other formats or sources (e.g. Parquet, JDBC).\n",
    "- **Flexibility**: By using `option(\"header\", True)` and `csv(...)`, the code ensures easy extension to real-world datasets without relying on any company-specific configuration.\n",
    "\n",
    "This block forms the foundation for subsequent data cleaning and anomaly detection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, sum, min, max, countDistinct, count, when, abs, expr, desc, mean, stddev, current_timestamp, lit, udf, struct, first, monotonically_increasing_id, format_number, coalesce, log1p, regexp_replace\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AnomalyDetection\").getOrCreate()\n",
    "\n",
    "# Get widget values\n",
    "file_pattern = \"sample_data.csv\"\n",
    "\n",
    "# Read in table\n",
    "main_df = spark.read.table(delta_table)\n",
    "print(f\"Using file: {file_pattern} for anomaly detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d2a09",
   "metadata": {},
   "source": [
    "## Cleaning the `TOTAL_SPEND_USD` Field\n",
    "\n",
    "This block focuses on cleaning and standardising the `TOTAL_SPEND_USD` field in the dataset, which is crucial for consistent analysis.\n",
    "\n",
    "- **Step 1**: Removes all characters except digits, dots, and parentheses using regular expressions. This step ensures only numeric values and potential negative indicators (parentheses) remain.\n",
    "- **Step 2**: Removes any trailing periods that might interfere with numeric conversion.\n",
    "- **Step 3**: Strips parentheses without yet negating the value. This simplifies further processing.\n",
    "- **Step 4**: Replaces empty strings with `null` to properly handle missing values in downstream analysis.\n",
    "- **Step 5**: Casts the cleaned value to `DoubleType`. If the original value had parentheses (indicating a negative value), it multiplies the value by -1 to reflect that negative sign.\n",
    "- **Step 6**: Drops the original `TOTAL_SPEND_USD` column and renames the cleaned version to preserve the column name consistency.\n",
    "\n",
    "This systematic cleaning process ensures that the spend data is properly formatted and ready for accurate anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning on TOTAL_SPEND_USD\n",
    "\n",
    "# Step 1: Remove all characters except digits, dots, and parentheses\n",
    "main_df = main_df.withColumn(\n",
    "    \"TOTAL_SPEND_USD_CLEAN\",\n",
    "    regexp_replace(col(\"TOTAL_SPEND_USD\"), \"[^0-9().]\", \"\")\n",
    ")\n",
    "\n",
    "# Step 2: Remove trailing period\n",
    "main_df = main_df.withColumn(\n",
    "    \"TOTAL_SPEND_USD_CLEAN\",\n",
    "    regexp_replace(col(\"TOTAL_SPEND_USD_CLEAN\"), \"\\\\.$\", \"\")\n",
    ")\n",
    "\n",
    "# Step 3: Remove parentheses if present (but don’t negate yet)\n",
    "main_df = main_df.withColumn(\n",
    "    \"TOTAL_SPEND_USD_CLEAN\",\n",
    "    when(\n",
    "        col(\"TOTAL_SPEND_USD\").rlike(\"\\\\(.*\\\\)\"),\n",
    "        regexp_replace(col(\"TOTAL_SPEND_USD_CLEAN\"), \"[()]\", \"\")\n",
    "    ).otherwise(col(\"TOTAL_SPEND_USD_CLEAN\"))\n",
    ")\n",
    "\n",
    "# Step 4: Replace empty strings with null\n",
    "main_df = main_df.withColumn(\n",
    "    \"TOTAL_SPEND_USD_CLEAN\",\n",
    "    when(col(\"TOTAL_SPEND_USD_CLEAN\") == \"\", None)\n",
    "    .otherwise(col(\"TOTAL_SPEND_USD_CLEAN\"))\n",
    ")\n",
    "\n",
    "# Step 5: Cast to Double and apply negation if originally in parentheses\n",
    "main_df = main_df.withColumn(\n",
    "    \"TOTAL_SPEND_USD_CLEAN\",\n",
    "    when(\n",
    "        col(\"TOTAL_SPEND_USD\").rlike(\"\\\\(.*\\\\)\"),\n",
    "        col(\"TOTAL_SPEND_USD_CLEAN\").cast(DoubleType()) * -1\n",
    "    ).otherwise(col(\"TOTAL_SPEND_USD_CLEAN\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# Step 6: Replace original column\n",
    "main_df = main_df.drop(\"TOTAL_SPEND_USD\") \\\n",
    "                 .withColumnRenamed(\"TOTAL_SPEND_USD_CLEAN\", \"TOTAL_SPEND_USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884b4f7",
   "metadata": {},
   "source": [
    "## Binning and Visualising Spend Data\n",
    "\n",
    "This block bins the cleaned spend data (`TOTAL_SPEND_USD`) into predefined ranges and visualises its distribution.\n",
    "\n",
    "- **Binning**: The `TOTAL_SPEND_USD` column is categorised into bins (e.g. `< $1K`, `$1K–$5K`, etc.) using PySpark’s `when` and `otherwise` functions. This approach helps to summarise spending behaviour at different levels.\n",
    "- **Grouping and Counting**: The dataset is grouped by spend bins, and the count of records in each bin is computed. The results are sorted in descending order to highlight the most populated bins.\n",
    "- **Data Conversion**: The grouped Spark DataFrame is converted into Pandas for easy plotting.\n",
    "- **Visualisation**: A bar chart using Matplotlib shows the distribution of records across spend bins. This helps identify spend concentration and provides an overview of data spread, essential for anomaly detection insights.\n",
    "\n",
    "This step helps in understanding the data distribution before applying any anomaly detection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b97436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define spend bins\n",
    "main_df = main_df.withColumn(\"Spend_Bin\", when(col(\"TOTAL_SPEND_USD\") < 1000, \"< $1K\")\n",
    "    .when(col(\"TOTAL_SPEND_USD\") < 5000, \"$1K–$5K\")\n",
    "    .when(col(\"TOTAL_SPEND_USD\") < 10000, \"$5K–$10K\")\n",
    "    .when(col(\"TOTAL_SPEND_USD\") < 50000, \"$10K–$50K\")\n",
    "    .when(col(\"TOTAL_SPEND_USD\") < 100000, \"$50K–$100K\")\n",
    "    .otherwise(\">= $100K\"))\n",
    "\n",
    "# Group by bin\n",
    "binned_spend_df = main_df.groupBy(\"Spend_Bin\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Convert to pandas df\n",
    "binned_pd = binned_spend_df.toPandas()\n",
    "\n",
    "spend_pd = main_df.select(\"TOTAL_SPEND_USD\").dropna().toPandas()\n",
    "\n",
    "# Bar chart to see distribution of data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(binned_pd[\"Spend_Bin\"], binned_pd[\"count\"])\n",
    "plt.xlabel(\"Spend Bin\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.title(\"Distribution of TOTAL_SPEND_USD\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c821a",
   "metadata": {},
   "source": [
    "## Data Aggregation\n",
    "\n",
    "This block focuses on aggregating transactional data at the purchase order (PO) level, preparing it for anomaly detection.\n",
    "\n",
    "- **Casting Spend to Numeric**: Ensures that the `TOTAL_SPEND_USD` column is properly cast to a `DoubleType`, which is essential for accurate aggregation.\n",
    "- **Distinct PO Count**: Counts the number of unique purchase orders in the dataset by considering `PURCHASE_ORDER_NUM` and `SUPPLIER_NAME`. This helps establish a baseline for overall data volume.\n",
    "- **Total Spend Calculation**: Computes the total spend across all records, providing context for spend distribution analysis.\n",
    "- **PO-Level Aggregation**: Groups the data by `PURCHASE_ORDER_NUM` and `SUPPLIER_NAME`, aggregating:\n",
    "  - Number of distinct line items (`LINE_ITEM_COUNT`).\n",
    "  - Total spend for each PO (`SUM_SPEND_USD`).\n",
    "  - Earliest and latest purchase and received dates.\n",
    "- **Natural Log Transformation**: Adds a `LOG_SUM_SPEND_USD` column using a log transformation (`log1p`) to reduce skew in the spend distribution, which is he_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8074c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "\n",
    "# Change spend column to numeric type\n",
    "main_df = main_df.withColumn(\"TOTAL_SPEND_USD\", col(\"TOTAL_SPEND_USD\").cast(DoubleType()))\n",
    "\n",
    "# Count distinct POs based on PURCHASE_ORDER_NUM, SUPPLIER_NAME, and DATA_SOURCE\n",
    "distinct_po_count = main_df.select(\"PURCHASE_ORDER_NUM\", \"SUPPLIER_NAME\") \\\n",
    "    .distinct() \\\n",
    "    .count()\n",
    "\n",
    "# Calculate total spend in file\n",
    "total_spend_in_file = main_df.select(sum(col(\"TOTAL_SPEND_USD\"))).collect()[0][0]\n",
    "\n",
    "# Print distinct purchase order count and total spend\n",
    "print(f\"Count of distinct POs in file (aggregated by PURCHASE_ORDER_NUM AND SUPPLIER_NAME): {distinct_po_count}\")\n",
    "print(f\"Total spend in file: ${total_spend_in_file}\")\n",
    "\n",
    "# PO-Level Aggregation\n",
    "po_agg_df = main_df.groupBy(\"PURCHASE_ORDER_NUM\", \"SUPPLIER_NAME\").agg(\n",
    "    countDistinct(col(\"PURCHASE_ORDER_LINE_NUM\")).alias(\"LINE_ITEM_COUNT\"),\n",
    "    sum(col(\"TOTAL_SPEND_USD\")).alias(\"SUM_SPEND_USD\"),\n",
    "    min(col(\"PURCHASE_ORDER_DATE\")).alias(\"EARLIEST_PO_DATE\"),\n",
    "    max(col(\"PURCHASE_ORDER_DATE\")).alias(\"LATEST_PO_DATE\"),\n",
    "    min(col(\"RECEIVED_DATE\")).alias(\"EARLIEST_RECEIVED_DATE\"),\n",
    "    max(col(\"RECEIVED_DATE\")).alias(\"LATEST_RECEIVED_DATE\")\n",
    ")\n",
    "\n",
    "# Add natural log transformation column to reduce skew in spend distribution\n",
    "po_agg_df = po_agg_df.withColumn(\"LOG_SUM_SPEND_USD\", log1p(col(\"SUM_SPEND_USD\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b8c4c",
   "metadata": {},
   "source": [
    "## Z-Score Anomaly Detection\n",
    "\n",
    "This block applies Z-Score anomaly detection to the aggregated spend data.\n",
    "\n",
    "- **Threshold Configuration**: Defines a threshold value for the Z-Score, typically around 3.0 for strong outlier detection.\n",
    "- **Mean and Standard Deviation Calculation**: Computes the mean and standard deviation of the target column (`LOG_SUM_SPEND_USD`) to understand its distribution.\n",
    "- **Z-Score Calculation**: Calculates the Z-Score for each record, measuring how many standard deviations a point is from the mean.\n",
    "- **Anomaly Flagging**: Flags records as anomalies if their absolute Z-Score exceeds the defined threshold. This helps identify unusually high or low spend amounts.\n",
    "- **Execution**: Applies the function to the aggregated PO dataset, enriching it with Z-Score values and anomaly flags.\n",
    "\n",
    "Z-Score is a simple yet effective statistical method to detect outliers based on distribution properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, stddev, when\n",
    "\n",
    "# Define Z-score threshold (configurable)\n",
    "z_score_threshold = 3.0 # default threshold\n",
    "\n",
    "# Z-score Anomaly Detection Function\n",
    "def zscore_anomaly_detection(df, column):\n",
    "    stats_df = df.select(mean(col(column)).alias(\"mean\"), stddev(col(column)).alias(\"stddev\"))\n",
    "    stats = stats_df.collect()[0] \n",
    "    \n",
    "    mean_val, stddev_val = stats[\"mean\"], stats[\"stddev\"]\n",
    "    \n",
    "    if mean_val is None or stddev_val is None or stddev_val == 0:\n",
    "        print(f\"Skipping Z-score anomaly detection for {column} (Invalid mean/stddev).\")\n",
    "        return df\n",
    "    \n",
    "    # Apply Z-score calculations\n",
    "    df = df.withColumn(\"Z_Score\", (col(column) - mean_val) / stddev_val)\n",
    "    df = df.withColumn(\"is_anomaly_zscore\", when(abs(col(\"Z_Score\")) > z_score_threshold, True).otherwise(False))\n",
    "    print(f\"✅ Z-score anomaly detection completed.\")\n",
    "    return df\n",
    "\n",
    "# Execution\n",
    "po_agg_df = zscore_anomaly_detection(po_agg_df, \"LOG_SUM_SPEND_USD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42379d2",
   "metadata": {},
   "source": [
    "## Z-Score Summary and Visualisation\n",
    "\n",
    "This block summarises the results of Z-Score anomaly detection and visualises the Z-Score distribution.\n",
    "\n",
    "- **Summary Function**: \n",
    "  - Calculates total record count, the number of anomalies (records flagged as anomalies by the Z-Score method), and non-anomalies.\n",
    "  - Computes minimum, maximum, and mean Z-Score values to understand the data spread.\n",
    "  - Returns a summary DataFrame showing anomaly statistics, helpful for both quantitative and qualitative analysis.\n",
    "\n",
    "- **Display Results**: Prints the summary DataFrame for review.\n",
    "\n",
    "- **Visualisation**: \n",
    "  - Plots a histogram of Z-Scores using Matplotlib to show the distribution of values.\n",
    "  - Helps identify where anomalies lie in the context of the overall distribution.\n",
    "\n",
    "This step highlights the effectiveness of Z-Score in capturing unusual records and provides both a tabular and visual perspective on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score Summary\n",
    "def summarize_zscore_anomalies(df, column):\n",
    "    anomaly_col = \"is_anomaly_zscore\"\n",
    "    z_score_col = \"Z_Score\"\n",
    "\n",
    "    if anomaly_col in df.columns and z_score_col in df.columns:\n",
    "        total_count = df.count()\n",
    "        anomaly_count = df.filter(col(anomaly_col) == True).count()\n",
    "        non_anomaly_count = total_count - anomaly_count\n",
    "        anomaly_percentage = (anomaly_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "        # Aggregate statistics\n",
    "        stats = df.agg(\n",
    "            min(col(z_score_col)).alias(\"Min Z-Score\"),\n",
    "            max(col(z_score_col)).alias(\"Max Z-Score\"),\n",
    "            mean(col(z_score_col)).alias(\"Mean Z-Score\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        summary_data = [{\n",
    "            \"Column\": column,\n",
    "            \"Total Records\": total_count,\n",
    "            \"Anomalies\": anomaly_count,\n",
    "            \"Non-Anomalies\": non_anomaly_count,\n",
    "            \"Anomaly Percentage (%)\": anomaly_percentage,\n",
    "            \"Min Z-Score\": stats[\"Min Z-Score\"] if stats[\"Min Z-Score\"] is not None else \"N/A\",\n",
    "            \"Max Z-Score\": stats[\"Max Z-Score\"] if stats[\"Max Z-Score\"] is not None else \"N/A\",\n",
    "            \"Mean Z-Score\": stats[\"Mean Z-Score\"] if stats[\"Mean Z-Score\"] is not None else \"N/A\"\n",
    "        }]\n",
    "\n",
    "        summary_df = spark.createDataFrame(summary_data)\n",
    "        summary_df.show(truncate=False)\n",
    "        return summary_df\n",
    "    else:\n",
    "        print(f\"Missing required columns in DataFrame: {anomaly_col}, {z_score_col}\")\n",
    "        return spark.createDataFrame([], schema=[\n",
    "            \"Column\", \"Total Records\", \"Anomalies\", \"Non-Anomalies\",\n",
    "            \"Anomaly Percentage (%)\", \"Min Z-Score\", \"Max Z-Score\", \"Mean Z-Score\"\n",
    "        ])\n",
    "\n",
    "# Generate summary\n",
    "anomaly_summary_df = summarize_zscore_anomalies(po_agg_df, \"LOG_SUM_SPEND_USD\")\n",
    "\n",
    "# Visualise Z-Score Distribution\n",
    "z_scores = po_agg_df.select(\"Z_Score\").dropna().toPandas()\n",
    "plt.hist(z_scores[\"Z_Score\"], bins=50)\n",
    "plt.title(\"Z-Score Distribution\")\n",
    "plt.xlabel(\"Z-Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ddbf1",
   "metadata": {},
   "source": [
    "## IQR Anomaly Detection\n",
    "\n",
    "This block applies the Interquartile Range (IQR) method to detect outliers in the spend data.\n",
    "\n",
    "- **Threshold Definition**: Sets the IQR threshold, typically 1.5, but adjustable based on business needs. This threshold determines the cutoff for identifying points as potential anomalies.\n",
    "- **Quantile Calculation**: Uses the approximate quantile method to efficiently calculate the first (Q1) and third quartiles (Q3) of the target column (`LOG_SUM_SPEND_USD`).\n",
    "- **IQR Score Calculation**: Computes a relative score that measures the distance from a point to the nearest quartile, scaled by the IQR. This provides a sense of how extreme an outlier is.\n",
    "- **Anomaly Flagging**: Flags records as anomalies if they fall below the lower bound (Q1 - threshold × IQR) or above the upper bound (Q3 + threshold × IQR).\n",
    "- **Execution**: Applies the function to the aggregated PO-level DataFrame, adding both the `IQR_Score` and `is_anomaly_iqr` columns for downstream analysis.\n",
    "\n",
    "IQR is a robust, non-parametric method that effectively identifies anomalies in skewed or non-normal data distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IQR threshold\n",
    "iqr_threshold = 1.5 # configurable\n",
    "\n",
    "# IQR Anomaly Detection Function\n",
    "def iqr_anomaly_detection(df: DataFrame, column: str) -> DataFrame:\n",
    "    quantiles = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    Q1, Q3 = quantiles[0], quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - iqr_threshold * IQR, Q3 + iqr_threshold * IQR\n",
    "\n",
    "    # Compute IQR Score (distance from nearest quartile divided by IQR)\n",
    "    df = df.withColumn(\n",
    "        \"IQR_Score\",\n",
    "        when(col(column) < Q1, (Q1 - col(column)) / IQR)\n",
    "        .when(col(column) > Q3, (col(column) - Q3) / IQR)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Flag anomalies\n",
    "    df = df.withColumn(\"is_anomaly_iqr\", (col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "    print(f\"✅ IQR anomaly detection completed.\")\n",
    "    return df\n",
    "\n",
    "# Execution\n",
    "po_agg_df = iqr_anomaly_detection(po_agg_df, \"LOG_SUM_SPEND_USD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb46ae",
   "metadata": {},
   "source": [
    "## IQR Anomaly Detection Summary\n",
    "\n",
    "This block summarises the results of the IQR-based anomaly detection.\n",
    "\n",
    "- **Summary Function**:\n",
    "  - Calculates the total number of records, the number of flagged anomalies, and the number of non-anomalies.\n",
    "  - Computes the anomaly rate as a percentage of the dataset.\n",
    "  - Aggregates key statistics including minimum, maximum, and mean IQR scores to provide insights into anomaly severity.\n",
    "\n",
    "- **Data Display**:\n",
    "  - Prints the summary DataFrame using `show()` for a clear tabular view of results.\n",
    "\n",
    "- **Execution**:\n",
    "  - Applies the summary function to the aggregated dataset, adding context to the IQR anomaly detection step.\n",
    "\n",
    "This step helps evaluate the effectiveness of the IQR method in capturing outliers and provides quick visibility into the distribution of anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise IQR anomaly detection results\n",
    "def summarize_iqr_anomalies(df, column):\n",
    "    anomaly_col = \"is_anomaly_iqr\"\n",
    "    iqr_score_col = \"IQR_Score\"\n",
    "\n",
    "    if anomaly_col in df.columns and iqr_score_col in df.columns:\n",
    "        total_count = df.count()\n",
    "        if total_count == 0:\n",
    "            print(\"No records found.\")\n",
    "            return spark.createDataFrame([], schema=[\n",
    "                \"Column\", \"Total Records\", \"Anomalies\", \"Non-Anomalies\",\n",
    "                \"Anomaly Percentage (%)\", \"Min IQR Score\", \"Max IQR Score\", \"Mean IQR Score\"\n",
    "            ])\n",
    "        \n",
    "        anomaly_count = df.filter(col(anomaly_col) == True).count()\n",
    "        non_anomaly_count = total_count - anomaly_count\n",
    "        anomaly_percentage = (anomaly_count / total_count) * 100\n",
    "\n",
    "        # Aggregate statistics\n",
    "        stats = df.agg(\n",
    "            min(col(iqr_score_col)).alias(\"Min IQR Score\"),\n",
    "            max(col(iqr_score_col)).alias(\"Max IQR Score\"),\n",
    "            mean(col(iqr_score_col)).alias(\"Mean IQR Score\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        summary_data = [{\n",
    "            \"Column\": column,\n",
    "            \"Total Records\": total_count,\n",
    "            \"Anomalies\": anomaly_count,\n",
    "            \"Non-Anomalies\": non_anomaly_count,\n",
    "            \"Anomaly Percentage (%)\": anomaly_percentage,\n",
    "            \"Min IQR Score\": stats[\"Min IQR Score\"] if stats[\"Min IQR Score\"] is not None else \"N/A\",\n",
    "            \"Max IQR Score\": stats[\"Max IQR Score\"] if stats[\"Max IQR Score\"] is not None else \"N/A\",\n",
    "            \"Mean IQR Score\": stats[\"Mean IQR Score\"] if stats[\"Mean IQR Score\"] is not None else \"N/A\"\n",
    "        }]\n",
    "\n",
    "        summary_df = spark.createDataFrame(summary_data)\n",
    "        summary_df.show(truncate=False)\n",
    "        return summary_df\n",
    "    else:\n",
    "        print(f\"Missing required columns in DataFrame: {anomaly_col}, {iqr_score_col}\")\n",
    "        return spark.createDataFrame([], schema=[\n",
    "            \"Column\", \"Total Records\", \"Anomalies\", \"Non-Anomalies\",\n",
    "            \"Anomaly Percentage (%)\", \"Min IQR Score\", \"Max IQR Score\", \"Mean IQR Score\"\n",
    "        ])\n",
    "\n",
    "# Generate IQR anomaly summary\n",
    "anomaly_summary_df = summarize_iqr_anomalies(po_agg_df, \"LOG_SUM_SPEND_USD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c437a2b",
   "metadata": {},
   "source": [
    "## Isolation Forest Anomaly Detection\n",
    "\n",
    "This block applies the Isolation Forest algorithm to detect outliers using a combination of numerical features.\n",
    "\n",
    "- **Feature Engineering**:\n",
    "  - `LOG_SUM_SPEND_USD`: A log transformation of spend to handle skewness.\n",
    "  - `SPEND_PER_LINE_ITEM`: Calculated spend per line item.\n",
    "  - `IS_LARGE_PO`: A binary indicator for purchase orders with exceptionally high spend (> $1,000,000)\n",
    "\n",
    "- **Unique Identifier**: Adds a `unique_id` column to facilitate merging results back into the Spark DataFrame.\n",
    "\n",
    "- **Pandas Conversion and Scaling**:\n",
    "  - Converts Spark DataFrame to Pandas for compatibility with Scikit-learn.\n",
    "  - Scales the features using `StandardScaler` for consistent performance.\n",
    "\n",
    "- **Model Training and Scoring**:\n",
    "  - Fits an Isolation Forest model with the specified contamination rate.\n",
    "  - Scores each record and flags as anomaly if its score exceeds the defined threshold.\n",
    "\n",
    "- **Merge Results**:\n",
    "  - Joins the results back into the Spark DataFrame, adding `IF_score_value` and `is_anomaly_isoforest` columns.\n",
    "\n",
    "This process leverages machine learning to detect multivariate anomalies that may not be evident with simpler methods like Z-Score or IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84100241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Isolation Forest Function\n",
    "def isolation_forest_anomaly_detection_multi(df, features, contamination=0.01):\n",
    "    \n",
    "    # Step 1: Feature Engineering\n",
    "    df = df.withColumn(\"LOG_SUM_SPEND_USD\", log1p(col(\"SUM_SPEND_USD\")))\n",
    "    df = df.withColumn(\n",
    "        \"SPEND_PER_LINE_ITEM\",\n",
    "        when(col(\"LINE_ITEM_COUNT\") != 0, col(\"SUM_SPEND_USD\") / col(\"LINE_ITEM_COUNT\")).otherwise(None)\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"IS_LARGE_PO\",\n",
    "        when(col(\"SUM_SPEND_USD\") > 1000000, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Step 2: Add unique ID\n",
    "    df = df.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "\n",
    "    # Step 3: Select relevant columns and convert to pandas\n",
    "    selected_cols = [\"unique_id\"] + features\n",
    "    df_pd = df.select(*selected_cols).toPandas()\n",
    "\n",
    "    # Step 4: Clean and scale\n",
    "    df_pd.dropna(subset=features, inplace=True)\n",
    "    df_pd[features] = df_pd[features].apply(pd.to_numeric, errors='coerce')\n",
    "    df_pd.dropna(subset=features, inplace=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_pd[features]), columns=features)\n",
    "\n",
    "    # Step 5: Fit Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    iso_forest.fit(df_scaled)\n",
    "\n",
    "    # Step 6: Score and flag\n",
    "    df_pd[\"IF_score_value\"] = -iso_forest.score_samples(df_scaled)\n",
    "    threshold = df_pd[\"IF_score_value\"].quantile(1 - contamination)\n",
    "    df_pd[\"is_anomaly_isoforest\"] = (df_pd[\"IF_score_value\"] > threshold).astype(bool)\n",
    "\n",
    "    # Step 7: Merge back to Spark\n",
    "    df_results = spark.createDataFrame(df_pd[[\"unique_id\", \"IF_score_value\", \"is_anomaly_isoforest\"]])\n",
    "    df = df.join(df_results, on=\"unique_id\", how=\"left\").drop(\"unique_id\")\n",
    "\n",
    "    print(f\"✅ Isolation Forest completed using: {features}\")\n",
    "    return df\n",
    "\n",
    "# Define features to use\n",
    "features_to_use = [\n",
    "    \"LOG_SUM_SPEND_USD\",\n",
    "    \"LINE_ITEM_COUNT\",\n",
    "    \"SPEND_PER_LINE_ITEM\",\n",
    "    \"IS_LARGE_PO\"\n",
    "]\n",
    "\n",
    "# Set contamination threshold manually\n",
    "iso_forest_threshold = 0.01  # Typically 1% or configurable\n",
    "\n",
    "# Execution\n",
    "po_agg_df = isolation_forest_anomaly_detection_multi(po_agg_df, features_to_use, iso_forest_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4625ec",
   "metadata": {},
   "source": [
    "## Isolation Forest Anomaly Summary\n",
    "\n",
    "This block summarises the results of Isolation Forest anomaly detection and visualises the distribution of anomaly scores.\n",
    "\n",
    "- **Summary Function**:\n",
    "  - Calculates total records, anomalies flagged, non-anomalies, and the anomaly rate.\n",
    "  - Computes minimum, maximum, and mean Isolation Forest scores to assess the severity and spread of anomalies.\n",
    "  - Returns a Spark DataFrame summarising the results for easy review.\n",
    "\n",
    "- **Data Display**:\n",
    "  - Prints the summary DataFrame using `.show()` to provide a clear, tabular summary of results.\n",
    "\n",
    "- **Visualisation**:\n",
    "  - Plots a histogram of Isolation Forest scores using Matplotlib, offering a quick look at score distribution and helping interpret thresholds and contamination rates.\n",
    "\n",
    "This step complements other anomaly detection methods (e.g. Z-Score, IQR) by adding a multivariate machine learning perspective to the anomaly detection process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f896f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise Isolation Forest anomaly detection results\n",
    "def summarize_isolation_forest_anomalies(df, column):\n",
    "    anomaly_col = \"is_anomaly_isoforest\"\n",
    "    if_score_col = \"IF_score_value\"\n",
    "\n",
    "    if anomaly_col in df.columns and if_score_col in df.columns: \n",
    "        total_count = df.count()\n",
    "        if total_count == 0:\n",
    "            print(\"No records found.\")\n",
    "            return spark.createDataFrame([], schema=[\n",
    "                \"Column\", \"Total Records\", \"Anomalies\", \"Non-Anomalies\",\n",
    "                \"Anomaly Percentage (%)\", \"Min IF Score\", \"Max IF Score\", \"Mean IF Score\"\n",
    "            ])\n",
    "        \n",
    "        anomaly_count = df.filter(col(anomaly_col) == True).count()\n",
    "        non_anomaly_count = total_count - anomaly_count\n",
    "        anomaly_percentage = (anomaly_count / total_count) * 100\n",
    "\n",
    "        # Aggregate statistics\n",
    "        stats = df.agg(\n",
    "            min(col(if_score_col)).alias(\"Min IF Score\"),\n",
    "            max(col(if_score_col)).alias(\"Max IF Score\"),\n",
    "            mean(col(if_score_col)).alias(\"Mean IF Score\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        summary_data = [{\n",
    "            \"Column\": column,\n",
    "            \"Total Records\": total_count,\n",
    "            \"Anomalies\": anomaly_count,\n",
    "            \"Non-Anomalies\": non_anomaly_count,\n",
    "            \"Anomaly Percentage (%)\": anomaly_percentage,\n",
    "            \"Min IF Score\": stats[\"Min IF Score\"] if stats[\"Min IF Score\"] is not None else \"N/A\",\n",
    "            \"Max IF Score\": stats[\"Max IF Score\"] if stats[\"Max IF Score\"] is not None else \"N/A\",\n",
    "            \"Mean IF Score\": stats[\"Mean IF Score\"] if stats[\"Mean IF Score\"] is not None else \"N/A\"\n",
    "        }]\n",
    "\n",
    "        summary_df = spark.createDataFrame(summary_data)\n",
    "        summary_df.show(truncate=False)\n",
    "        return summary_df\n",
    "    else:\n",
    "        print(f\"Missing required columns in DataFrame: {anomaly_col}, {if_score_col}\")\n",
    "        return spark.createDataFrame([], schema=[\n",
    "            \"Column\", \"Total Records\", \"Anomalies\", \"Non-Anomalies\",\n",
    "            \"Anomaly Percentage (%)\", \"Min IF Score\", \"Max IF Score\", \"Mean IF Score\"\n",
    "        ])\n",
    "\n",
    "# Generate Isolation Forest anomaly summary\n",
    "anomaly_summary_df = summarize_isolation_forest_anomalies(po_agg_df, \"LOG_SUM_SPEND_USD\")\n",
    "\n",
    "# Visualise Isolation Forest scores\n",
    "if_scores = po_agg_df.select(\"IF_score_value\").dropna().toPandas()\n",
    "plt.hist(if_scores[\"IF_score_value\"], bins=50)\n",
    "plt.title(\"Isolation Forest Score Distribution\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08dbf82",
   "metadata": {},
   "source": [
    "## Weighted Anomaly Scoring\n",
    "\n",
    "This block calculates a weighted anomaly score by combining multiple detection methods and a spend-based component.\n",
    "\n",
    "- **Raw Weights**:\n",
    "  - Assigns initial weights to each anomaly detection method:\n",
    "    - `is_anomaly_zscore`: 30%\n",
    "    - `is_anomaly_iqr`: 20%\n",
    "    - `is_anomaly_isoforest`: 30%\n",
    "    - `spend_magnitude`: 20%\n",
    "  - Normalises these weights to ensure they sum to 1.0.\n",
    "\n",
    "- **Model Votes**:\n",
    "  - Each anomaly detection flag (`is_anomaly_zscore`, `is_anomaly_iqr`, and `is_anomaly_isoforest`) is converted to a numeric indicator and weighted accordingly.\n",
    "\n",
    "- **Spend Component**:\n",
    "  - Incorporates the relative spend magnitude by comparing each record’s spend to the 95th percentile.\n",
    "  - This component captures unusually high spend behaviour that may not be flagged by individual models.\n",
    "\n",
    "- **Score Calculation**:\n",
    "  - Combines all weighted components into a single `weighted_score` column.\n",
    "\n",
    "This composite score helps prioritise records for review by combining multiple detection methods with domain-relevant metrics like spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4eaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Score w/ Spend\n",
    "def apply_weighted_score(df):\n",
    "    # Raw weights\n",
    "    raw_weights = {\n",
    "        \"is_anomaly_zscore\": 30,\n",
    "        \"is_anomaly_iqr\": 20,\n",
    "        \"is_anomaly_isoforest\": 30,\n",
    "        \"spend_magnitude\": 20\n",
    "    }\n",
    "\n",
    "    # Normalize weights\n",
    "    total_weight = 0\n",
    "    for v in raw_weights.values():\n",
    "        total_weight += v\n",
    "    weights = {key: value / total_weight for key, value in raw_weights.items()}\n",
    "    print(\"Total Weight:\", total_weight)\n",
    "\n",
    "    # Set column list\n",
    "    anomaly_columns = [\"is_anomaly_zscore\", \"is_anomaly_iqr\", \"is_anomaly_isoforest\"]\n",
    "\n",
    "    # Build the anomaly flag score expression\n",
    "    model_exprs = [\n",
    "        f\"{weights[col]} * CAST(COALESCE(CAST({col} AS INT), 0) AS DOUBLE)\"\n",
    "        for col in anomaly_columns\n",
    "    ]\n",
    "\n",
    "    # Get the 95th percentile value for spend\n",
    "    p95_value = df.selectExpr(\"percentile_approx(SUM_SPEND_USD, 0.95)\").first()[0]\n",
    "\n",
    "    # Spend component\n",
    "    spend_expr = f\"{weights['spend_magnitude']} * LEAST(SUM_SPEND_USD / {p95_value}, 1.0)\"\n",
    "\n",
    "    # Combine all into one string expression\n",
    "    full_expr = \" + \".join(model_exprs + [spend_expr])\n",
    "\n",
    "    # Return DataFrame with weighted score\n",
    "    return df.withColumn(\"weighted_score\", expr(full_expr))\n",
    "\n",
    "# Execution\n",
    "po_agg_df = apply_weighted_score(po_agg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef4b2e",
   "metadata": {},
   "source": [
    "## Model Agreement Bonus\n",
    "\n",
    "This block enhances the anomaly scoring by introducing a bonus mechanism based on the agreement between different detection models.\n",
    "\n",
    "- **Model Votes**:\n",
    "  - Sums the binary flags from the three detection methods (`is_anomaly_zscore`, `is_anomaly_iqr`, and `is_anomaly_isoforest`).\n",
    "  - This creates a `model_votes` column indicating how many models agreed on a given anomaly.\n",
    "\n",
    "- **Agreement Bonus**:\n",
    "  - If two models flag the same record as an anomaly, adds a bonus of 0.15 to the score.\n",
    "  - If all three models agree, adds a bonus of 0.25.\n",
    "  - This approach rewards consensus and highlights anomalies that multiple models identify.\n",
    "\n",
    "- **Final Scoring**:\n",
    "  - Combines the `weighted_score` with the `agreement_bonus` to produce a final `score_for_severity`.\n",
    "  - This score better captures both individual model insights and cross-model agreement.\n",
    "\n",
    "This strategy refines anomaly ranking by incorporating both model votes and individual detection weights, making it easier to prioritise anomalies for further investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Agreement Bonus\n",
    "\n",
    "# Add model_votes and bonus\n",
    "po_agg_df = po_agg_df.withColumn(\n",
    "    \"model_votes\",\n",
    "    expr(\"CAST(is_anomaly_zscore AS INT) + CAST(is_anomaly_iqr AS INT) + CAST(is_anomaly_isoforest AS INT)\")\n",
    ").withColumn(\n",
    "    \"agreement_bonus\",\n",
    "    when(col(\"model_votes\") == 2, 0.15)\n",
    "    .when(col(\"model_votes\") == 3, 0.25)\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "\n",
    "po_agg_df = po_agg_df.withColumn(\n",
    "    \"score_for_severity\",\n",
    "    col(\"weighted_score\") + col(\"agreement_bonus\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675e840",
   "metadata": {},
   "source": [
    "## Severity Classification\n",
    "\n",
    "This block classifies records into severity levels based on their anomaly scores.\n",
    "\n",
    "- **Classification Function**:\n",
    "  - Defines thresholds to classify each record into one of four severity categories:\n",
    "    - `'None'`: Score less than 0.4, indicating normal behaviour.\n",
    "    - `'Mild'`: Score between 0.4 and 0.65, indicating potential anomalies.\n",
    "    - `'High'`: Score between 0.65 and 0.85, indicating likely anomalies.\n",
    "    - `'Severe'`: Score of 0.85 or higher, indicating strong anomalies.\n",
    "\n",
    "- **Implementation**:\n",
    "  - Uses PySpark’s `expr()` function to define a CASE statement, creating a new `severity` column.\n",
    "  - Applies the function to the anomaly DataFrame, enriching it with an easy-to-interpret severity label.\n",
    "\n",
    "This classification step helps prioritise records by severity, enabling more focused investigation and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b838fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity\n",
    "def apply_severity_classification(df: DataFrame, score_col: str = \"weighted_score\") -> DataFrame:\n",
    "    return df.withColumn(\"severity\",\n",
    "        expr(f\"\"\"\n",
    "            CASE\n",
    "                WHEN {score_col} < 0.4 THEN 'None'\n",
    "                WHEN {score_col} >= 0.4 AND {score_col} < 0.65 THEN 'Mild'\n",
    "                WHEN {score_col} >= 0.65 AND {score_col} < 0.85 THEN 'High'\n",
    "                WHEN {score_col} >= 0.85 THEN 'Severe'\n",
    "            END\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "\n",
    "# Execution\n",
    "po_agg_df = apply_severity_classification(po_agg_df, score_col=\"score_for_severity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80d1f3",
   "metadata": {},
   "source": [
    "## Generating Anomaly Insights\n",
    "\n",
    "This block generates textual insights for each record based on which anomaly detection methods flagged it as anomalous.\n",
    "\n",
    "- **Insight Generation Function**:\n",
    "  - For each record, extracts relevant fields like purchase order number (`PURCHASE_ORDER_NUM`), supplier name (`SUPPLIER_NAME`), and total spend.\n",
    "  - Checks each anomaly detection flag (`is_anomaly_zscore`, `is_anomaly_iqr`, `is_anomaly_isoforest`).\n",
    "  - For each flagged anomaly type:\n",
    "    - Retrieves the corresponding score (Z-Score, IQR Score, Isolation Forest Score).\n",
    "    - Appends a descriptive insight to a list.\n",
    "  - Joins all applicable insights into a single string. If no anomalies are detected, returns `\"No anomalies detected.\"`.\n",
    "\n",
    "- **Application to DataFrame**:\n",
    "  - Registers the insight function as a PySpark UDF.\n",
    "  - Applies the UDF to the DataFrame, creating a new `insight` column that summarises the anomalies detected for each record.\n",
    "\n",
    "This step makes the anomaly results more interpretable and provides context for stakeholders, enabling more effective decision-making and anomaly investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "def generate_insight(row):\n",
    "    insights = []\n",
    "\n",
    "    po_num = getattr(row, \"PURCHASE_ORDER_NUM\", \"UNKNOWN\")\n",
    "    supplier = getattr(row, \"SUPPLIER_NAME\", \"UNKNOWN\")\n",
    "\n",
    "    try:\n",
    "        spend = round(float(getattr(row, \"SUM_SPEND_USD\", 0) or 0))\n",
    "    except:\n",
    "        spend = 0\n",
    "\n",
    "    if getattr(row, \"is_anomaly_zscore\", False):\n",
    "        try:\n",
    "            z = round(float(getattr(row, \"Z_Score\", 0) or 0), 2)\n",
    "            insights.append(f\"PO {po_num} from {supplier} was flagged by Z-score (Amount: ${spend}, Score: {z}).\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if getattr(row, \"is_anomaly_iqr\", False):\n",
    "        try:\n",
    "            iqr = round(float(getattr(row, \"IQR_Score\", 0) or 0), 2)\n",
    "            insights.append(f\"PO {po_num} from {supplier} was flagged by IQR (Amount: ${spend}, Score: {iqr}).\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if getattr(row, \"is_anomaly_isoforest\", False):\n",
    "        try:\n",
    "            iso = round(float(getattr(row, \"IF_score_value\", 0) or 0), 2)\n",
    "            insights.append(f\"PO {po_num} from {supplier} was flagged by Isolation Forest (Amount: ${spend}, Score: {iso}).\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return \" \".join(insights) if insights else \"No anomalies detected.\"\n",
    "\n",
    "\n",
    "# Apply insights function\n",
    "def apply_insights(df: DataFrame) -> DataFrame:\n",
    "    generate_insight_udf = udf(generate_insight, StringType())\n",
    "    return df.withColumn(\"insight\", generate_insight_udf(struct([df[x] for x in df.columns])))\n",
    "\n",
    "# Execution\n",
    "po_agg_df = apply_insights(po_agg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb5569",
   "metadata": {},
   "source": [
    "## Outlier Summary\n",
    "\n",
    "This block consolidates the results of anomaly detection and presents a summary of key metrics.\n",
    "\n",
    "- **Data Preparation**:\n",
    "  - Ensures numerical columns (e.g. `weighted_score`, `Z_Score`, `IQR_Score`, `IF_score_value`) are properly cast to double precision for consistency.\n",
    "  \n",
    "- **Counting Anomalies**:\n",
    "  - Counts the number of flagged anomalies for each detection method (Z-Score, IQR, Isolation Forest) and by severity level (Severe, High, Mild).\n",
    "\n",
    "- **Spend Analysis**:\n",
    "  - Calculates the total spend per severity level, offering insights into the potential financial impact of anomalies.\n",
    "  - Computes percentages of spend per severity relative to total spend to prioritise investigations.\n",
    "\n",
    "- **Final Summary**:\n",
    "  - Prints a comprehensive summary of anomaly counts, detection percentages, and spend impact.\n",
    "\n",
    "This step ties the entire pipeline together by translating detection outputs into actionable insights for review and prioritisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "\n",
    "# --- Cast numeric columns ---\n",
    "numeric_cols = [\"weighted_score\", \"Z_Score\", \"IQR_Score\", \"IF_score_value\"]\n",
    "for col_name in numeric_cols:\n",
    "    po_agg_df = po_agg_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "# --- Count anomaly flags ---\n",
    "anomalies_z = po_agg_df.filter(col(\"is_anomaly_zscore\")).count()\n",
    "anomalies_iqr = po_agg_df.filter(col(\"is_anomaly_iqr\")).count()\n",
    "anomalies_isoforest = po_agg_df.filter(col(\"is_anomaly_isoforest\")).count()\n",
    "severe_anomalies = po_agg_df.filter(col(\"severity\") == 'Severe').count()\n",
    "high_anomalies = po_agg_df.filter(col(\"severity\") == 'High').count()\n",
    "mild_anomalies = po_agg_df.filter(col(\"severity\") == 'Mild').count()\n",
    "\n",
    "# --- Calculate detection percentages ---\n",
    "percent_z = (anomalies_z / distinct_po_count) * 100\n",
    "percent_iqr = (anomalies_iqr / distinct_po_count) * 100\n",
    "percent_isoforest = (anomalies_isoforest / distinct_po_count) * 100\n",
    "percent_severe = (severe_anomalies / distinct_po_count) * 100\n",
    "percent_high = (high_anomalies / distinct_po_count) * 100\n",
    "percent_mild = (mild_anomalies / distinct_po_count) * 100\n",
    "\n",
    "# --- Total row and spend calculations ---\n",
    "total_rows = main_df.count()\n",
    "total_spend = main_df.agg(sum(\"TOTAL_SPEND_USD\")).collect()[0][0] or 0\n",
    "total_spend = round(total_spend, 2)\n",
    "\n",
    "# --- Helper function ---\n",
    "def get_anomaly_spend(df, severity_level):\n",
    "    spend = (\n",
    "        df\n",
    "        .filter(col(\"severity\") == severity_level)\n",
    "        .dropDuplicates([\"PURCHASE_ORDER_NUM\"])\n",
    "        .agg(sum(\"SUM_SPEND_USD\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    return round(spend or 0, 2)\n",
    "\n",
    "# --- Spend by severity level ---\n",
    "severe_anomaly_spend = get_anomaly_spend(po_agg_df, \"Severe\")\n",
    "high_anomaly_spend = get_anomaly_spend(po_agg_df, \"High\")\n",
    "mild_anomaly_spend = get_anomaly_spend(po_agg_df, \"Mild\")\n",
    "\n",
    "# --- Percentage of spend per severity level ---\n",
    "severe_anomaly_pct = (severe_anomaly_spend / total_spend) * 100 if total_spend else 0\n",
    "high_anomaly_pct = (high_anomaly_spend / total_spend) * 100 if total_spend else 0\n",
    "mild_anomaly_pct = (mild_anomaly_spend / total_spend) * 100 if total_spend else 0\n",
    "\n",
    "# --- Investigate anomalies with percent > 100 ---\n",
    "percent_dict = {\n",
    "    \"percent_z\": percent_z,\n",
    "    \"percent_iqr\": percent_iqr,\n",
    "    \"percent_isoforest\": percent_isoforest,\n",
    "    \"percent_severe\": percent_severe,\n",
    "    \"percent_high\": percent_high,\n",
    "    \"percent_mild\": percent_mild,\n",
    "    \"severe_anomaly_pct\": severe_anomaly_pct,\n",
    "    \"high_anomaly_pct\": high_anomaly_pct,\n",
    "    \"mild_anomaly_pct\": mild_anomaly_pct\n",
    "}\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n=== Anomaly Detection Summary ===\")\n",
    "print(f\"Total Records: {total_rows}\")\n",
    "print(f\"Total Spend: {total_spend}\")\n",
    "print(f\"Z-Score Outliers: {anomalies_z} ({percent_z:.2f}%)\")\n",
    "print(f\"IQR Outliers: {anomalies_iqr} ({percent_iqr:.2f}%)\")\n",
    "print(f\"Isolation Forest Outliers: {anomalies_isoforest} ({percent_isoforest:.2f}%)\")\n",
    "print(f\"Severe Outliers: {severe_anomalies} ({percent_severe:.2f}%)\")\n",
    "print(f\"Severe Outlier Spend: {severe_anomaly_spend}\")\n",
    "print(f\"High Outliers: {high_anomalies} ({percent_high:.2f}%)\")\n",
    "print(f\"High Outlier Spend: {high_anomaly_spend}\")\n",
    "print(f\"Mild Outliers: {mild_anomalies} ({percent_mild:.2f}%)\")\n",
    "print(f\"Mild Outlier Spend: {mild_anomaly_spend}\")\n",
    "print(\"==========================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18f8b6",
   "metadata": {},
   "source": [
    "## Automated Email Report\n",
    "\n",
    "This block generates an HTML summary report that can be embedded in an email for stakeholders.\n",
    "\n",
    "- **Function Overview**:\n",
    "  - Accepts key summary metrics: total records, total spend, distinct PO count, and anomaly spend/percentages by severity.\n",
    "  - Formats the results into an HTML table using in-line CSS for styling.\n",
    "\n",
    "- **Highlights**:\n",
    "  - Clear tabular layout showing anomaly breakdown by spend category: Mild, High, and Severe.\n",
    "  - Includes helpful formatting (thousands separators, currency formatting, percentages).\n",
    "  - Provides a consistent, professional presentation for sharing insights with non-technical stakeholders.\n",
    "\n",
    "- **Execution**:\n",
    "  - Calls the function with current summary statistics to generate the final HTML string.\n",
    "\n",
    "This reporting step transforms numerical insights into an executive-friendly format, enabling quick review and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated email report\n",
    "def generate_exec_html_email(file_pattern, total_rows, total_spend, distinct_po_count,\n",
    "                              mild_anomaly_spend, mild_anomaly_pct,\n",
    "                              high_anomaly_spend, high_anomaly_pct,\n",
    "                              severe_anomaly_spend, severe_anomaly_pct):\n",
    "\n",
    "    total_spend = total_spend or 0.0\n",
    "    mild_anomaly_spend = mild_anomaly_spend or 0.0\n",
    "    high_anomaly_spend = high_anomaly_spend or 0.0\n",
    "    severe_anomaly_spend = severe_anomaly_spend or 0.0\n",
    "    mild_anomaly_pct = mild_anomaly_pct or 0.0\n",
    "    high_anomaly_pct = high_anomaly_pct or 0.0\n",
    "    severe_anomaly_pct = severe_anomaly_pct or 0.0\n",
    "\n",
    "    report = f\"\"\"\n",
    "    <html>\n",
    "    <body style=\"font-family: Arial, sans-serif;\">\n",
    "        <h2 style=\"color: #003366;\">Outlier Detection Summary</h2>\n",
    "\n",
    "        <table cellpadding=\"6\" cellspacing=\"0\" style=\"border-collapse: collapse; font-size: 14px; width: 60%;\">\n",
    "            <tr style=\"background-color: #d9e1f2; font-weight: bold;\">\n",
    "                <th style=\"border: 1px solid #999;\">CATEGORY</th>\n",
    "                <th style=\"border: 1px solid #999;\">VALUE</th>\n",
    "                <th style=\"border: 1px solid #999;\">SPEND %</th>\n",
    "            </tr>\n",
    "            <tr><td style=\"border: 1px solid #999;\">Extract</td><td colspan=\"2\" style=\"border: 1px solid #999;\">{file_pattern}</td></tr>\n",
    "            <tr><td style=\"border: 1px solid #999;\">Total records</td><td style=\"border: 1px solid #999;\">{total_rows:,}</td><td style=\"border: 1px solid #999;\">-</td></tr>\n",
    "            <tr><td style=\"border: 1px solid #999;\">Total spend</td><td style=\"border: 1px solid #999;\">${total_spend:,.2f}</td><td style=\"border: 1px solid #999;\">100%</td></tr>\n",
    "            <tr><td style=\"border: 1px solid #999;\">PO count</td><td style=\"border: 1px solid #999;\">{distinct_po_count}</td><td style=\"border: 1px solid #999;\">-</td></tr>\n",
    "            <tr style=\"background-color: #fff8dc;\"><td style=\"border: 1px solid #999;\">Mild outliers</td><td style=\"border: 1px solid #999;\">${mild_anomaly_spend:,.2f}</td><td style=\"border: 1px solid #999;\">{mild_anomaly_pct:.3f}%</td></tr>\n",
    "            <tr style=\"background-color: #ffe4e1;\"><td style=\"border: 1px solid #999;\">High outliers</td><td style=\"border: 1px solid #999;\">${high_anomaly_spend:,.2f}</td><td style=\"border: 1px solid #999;\">{high_anomaly_pct:.3f}%</td></tr>\n",
    "            <tr style=\"background-color: #f08080;\"><td style=\"border: 1px solid #999;\">Severe outliers</td><td style=\"border: 1px solid #999;\">${severe_anomaly_spend:,.2f}</td><td style=\"border: 1px solid #999;\">{severe_anomaly_pct:.3f}%</td></tr>\n",
    "        </table>\n",
    "\n",
    "        <p style=\"font-size: 12px; color: #666; margin-top: 20px;\">\n",
    "            This report summarises flagged anomalies by spend category.\n",
    "        </p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return report\n",
    "\n",
    "# Generate the report with new values\n",
    "report = generate_exec_html_email(\n",
    "    file_pattern, total_rows, total_spend, distinct_po_count, mild_anomaly_spend, mild_anomaly_pct, high_anomaly_spend, high_anomaly_pct,severe_anomaly_spend, severe_anomaly_pct\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18a9e0",
   "metadata": {},
   "source": [
    "## Automated Email Report with Microsoft Graph API\n",
    "\n",
    "This block handles the final step of the pipeline: saving the anomaly report and sending it via email using the Microsoft Graph API.\n",
    "\n",
    "- **Setup**:\n",
    "  - Uses the `requests` library to interact with the Microsoft Graph API for authentication and email sending.\n",
    "  - Encodes the CSV file in Base64 if an attachment is required.\n",
    "\n",
    "- **Access Token**:\n",
    "  - Retrieves an access token using client credentials\n",
    "\n",
    "- **Email Composition**:\n",
    "  - Builds the email payload, including:\n",
    "    - Subject line with the file name.\n",
    "    - HTML body with the anomaly summary.\n",
    "    - Optional attachment of the CSV report.\n",
    "\n",
    "- **Sending**:\n",
    "  - Makes a POST request to the Graph API `/sendMail` endpoint.\n",
    "  - Prints success or error messages for visibility.\n",
    "\n",
    "- **Data Export**:\n",
    "  - Saves the anomaly DataFrame to a local CSV file, providing a record for future reference.\n",
    "\n",
    "This step ensures that stakeholders can receive the anomaly detection results automatically, reducing manual intervention and streamlining communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501301ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import base64\n",
    "import glob\n",
    "\n",
    "# Set email recipient\n",
    "email_recipient = \"example@email.com\"\n",
    "\n",
    "# Function to get access token\n",
    "def get_access_token():\n",
    "    url = \"https://login.microsoftonline.com/YOUR_TENANT_ID/oauth2/v2.0/token\"\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    body = {\n",
    "        'client_id': 'YOUR_CLIENT_ID',\n",
    "        'client_secret': 'YOUR_CLIENT_SECRET',\n",
    "        'grant_type': 'client_credentials',\n",
    "        'scope': 'https://graph.microsoft.com/.default'\n",
    "    }\n",
    "    response = requests.post(url, data=body, headers=headers)\n",
    "    return response.json().get('access_token')\n",
    "\n",
    "# Function to encode file\n",
    "def encode_file_to_base64(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        return base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Function to send email\n",
    "def send_email(report, file_pattern, attachment_path=None):\n",
    "    access_token = get_access_token()\n",
    "    url = \"https://graph.microsoft.com/v1.0/users/YOUR_EMAIL/sendMail\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "    email_data = {\n",
    "        \"message\": {\n",
    "            \"subject\": f\"Outlier Report for File: {file_pattern}\",\n",
    "            \"body\": {\"contentType\": \"HTML\", \"content\": report},\n",
    "            \"toRecipients\": [{\"emailAddress\": {\"address\": email_recipient}}]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if attachment_path:\n",
    "        encoded_attachment = encode_file_to_base64(attachment_path)\n",
    "        email_data[\"message\"][\"attachments\"] = [{\n",
    "            \"@odata.type\": \"#microsoft.graph.fileAttachment\",\n",
    "            \"name\": os.path.basename(attachment_path),\n",
    "            \"contentType\": \"text/csv\",\n",
    "            \"contentBytes\": encoded_attachment\n",
    "        }]\n",
    "\n",
    "    response = requests.post(url, json=email_data, headers=headers)\n",
    "    if response.status_code == 202:\n",
    "        print(\"✅ Email sent successfully!\" + (\" (With Attachment)\" if attachment_path else \"\"))\n",
    "    else:\n",
    "        print(f\"❌ Failed to send email: {response.text}\")\n",
    "\n",
    "# Save DataFrame to local CSV\n",
    "output_csv_path = f\"output/{file_pattern}_outlier_report.csv\"\n",
    "po_agg_df.toPandas().to_csv(output_csv_path, index=False)\n",
    "print(f\"✅ CSV file saved at: {output_csv_path}\")\n",
    "\n",
    "# Send email with report and attachment\n",
    "send_email(report, file_pattern, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e100e89",
   "metadata": {},
   "source": [
    "## Anomaly Detection Logging\n",
    "\n",
    "This block captures and logs key metrics from the anomaly detection process for record-keeping and analysis.\n",
    "\n",
    "- **Logging Setup**:\n",
    "  - Defines table names for storing anomaly logs and anomaly data, parameterised by the business unit.\n",
    "  - Uses Delta Lake for efficient, scalable storage and querying.\n",
    "\n",
    "- **Log Function**:\n",
    "  - Creates a DataFrame with summary metrics: file name, total rows, total spend, and anomaly counts for each method.\n",
    "  - Adds a timestamp column to track when the run occurred.\n",
    "  - Appends the log to the `high_level_anomaly_log` Delta table.\n",
    "\n",
    "- **Anomaly Data Export**:\n",
    "  - Appends the full anomaly detection results to a dedicated Delta table for the business unit.\n",
    "\n",
    "- **Audit Check**:\n",
    "  - Queries and displays the last 10 anomaly detection runs from the log table, providing quick access to recent results.\n",
    "\n",
    "This step ensures traceability and accountability for each anomaly detection run, supporting audit and review processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d354fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "\n",
    "# Set business unit\n",
    "business_unit = \"example_business_unit\"\n",
    "\n",
    "# Define logging table names\n",
    "log_table_name = \"anomaly_detection.high_level_anomaly_log\"\n",
    "export_table = f\"anomaly_detection.{business_unit}_file_anomalies\"\n",
    "\n",
    "def log_anomaly_detection_run(file_pattern, total_rows, total_spend, anomalies_z, anomalies_iqr, anomalies_isoforest, severe_anomalies):\n",
    "    \"\"\"\n",
    "    Logs anomaly detection metrics for a specific file into a Delta table.\n",
    "    \"\"\"\n",
    "    log_df = spark.createDataFrame([(\n",
    "        file_pattern, total_rows, total_spend, anomalies_z, anomalies_iqr, anomalies_isoforest, severe_anomalies\n",
    "    )], [\n",
    "        \"file_pattern\", \"total_rows\", \"total_spend\",\n",
    "        \"anomalies_z\", \"anomalies_iqr\", \"anomalies_isoforest\", \"severe_anomalies\"\n",
    "    ]).withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "    # Append to the table\n",
    "    log_df.write.format(\"delta\").mode(\"append\").saveAsTable(log_table_name)\n",
    "    print(f\"✅ Logged anomaly detection run for file: {file_pattern}\")\n",
    "\n",
    "# Save anomaly data to table\n",
    "po_agg_df.write.format(\"delta\").mode(\"append\").saveAsTable(export_table)\n",
    "print(f\"✅ Anomaly data written to table: {export_table}\")\n",
    "\n",
    "# Log anomaly detection run\n",
    "log_anomaly_detection_run(\n",
    "    file_pattern=file_pattern,\n",
    "    total_rows=total_rows,\n",
    "    total_spend=total_spend,\n",
    "    anomalies_z=anomalies_z,\n",
    "    anomalies_iqr=anomalies_iqr,\n",
    "    anomalies_isoforest=anomalies_isoforest,\n",
    "    severe_anomalies=severe_anomalies\n",
    ")\n",
    "\n",
    "# Show last 10 anomaly detection runs\n",
    "last_10_logs = spark.sql(f\"SELECT * FROM {log_table_name} ORDER BY timestamp DESC LIMIT 10\")\n",
    "last_10_logs.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
